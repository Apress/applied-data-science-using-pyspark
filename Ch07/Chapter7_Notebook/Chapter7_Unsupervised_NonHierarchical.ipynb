{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Hierarchical Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sparksession\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Clustering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.4 | packaged by conda-forge | (default, Jul 17 2020, 15:16:46) \n",
      "[GCC 7.5.0]\n",
      "Spark version: 3.0.0\n"
     ]
    }
   ],
   "source": [
    "# Print PySpark and Python versions\n",
    "import sys\n",
    "print('Python version: '+sys.version)\n",
    "print('Spark version: '+spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_location = \"cluster_data.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "\n",
    "\n",
    "df = spark.read.format(file_type)\\\n",
    ".option(\"inferSchema\", infer_schema)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".load(file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUST_ID: string (nullable = true)\n",
      " |-- BALANCE: string (nullable = true)\n",
      " |-- BALANCE_FREQUENCY: string (nullable = true)\n",
      " |-- PURCHASES: string (nullable = true)\n",
      " |-- ONEOFF_PURCHASES: string (nullable = true)\n",
      " |-- INSTALLMENTS_PURCHASES: string (nullable = true)\n",
      " |-- CASH_ADVANCE: string (nullable = true)\n",
      " |-- PURCHASES_FREQUENCY: string (nullable = true)\n",
      " |-- ONEOFF_PURCHASES_FREQUENCY: string (nullable = true)\n",
      " |-- PURCHASES_INSTALLMENTS_FREQUENCY: string (nullable = true)\n",
      " |-- CASH_ADVANCE_FREQUENCY: string (nullable = true)\n",
      " |-- CASH_ADVANCE_TRX: string (nullable = true)\n",
      " |-- PURCHASES_TRX: string (nullable = true)\n",
      " |-- CREDIT_LIMIT: string (nullable = true)\n",
      " |-- PAYMENTS: string (nullable = true)\n",
      " |-- MINIMUM_PAYMENTS: string (nullable = true)\n",
      " |-- PRC_FULL_PAYMENT: string (nullable = true)\n",
      " |-- TENURE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Metadata\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of records in the credit card dataset are 8950\n"
     ]
    }
   ],
   "source": [
    "#  Count data\n",
    "df.count()\n",
    "print('The total number of records in the credit card dataset are '+str(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting variable to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting multiple variables\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Identifying and assiging lists of variables  \n",
    "float_vars=list(set(df.columns) - set(['CUST_ID']))\n",
    "\n",
    "\n",
    "for column in float_vars:\n",
    "\tdf=df.withColumn(column,df[column].cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Shortlisting variables where mean imputation is required\n",
    "input_cols=list(set(df.columns) - set(['CUST_ID']))\n",
    "\n",
    "# Defining the imputer function\n",
    "imputer = Imputer(\n",
    "    inputCols=input_cols, \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in input_cols])\n",
    "\n",
    "# Applying the transformation\n",
    "df_imputed=imputer.fit(df).transform(df)\n",
    "\n",
    "# Dropping the original columns as we created the _imputed columns\n",
    "df_imputed=df_imputed.drop(*input_cols)\n",
    "\n",
    "# Renaming the input columns to original columns for consistency\n",
    "new_column_name_list= list(map(lambda x: x.replace(\"_imputed\", \"\"), df.columns))\n",
    "df_imputed = df_imputed.toDF(*new_column_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Listing the variables that are not required in the segmentation analysis\n",
    "ignore = ['CUST_ID']\n",
    "# creating vector of all features\n",
    "assembler = VectorAssembler(inputCols=[x for x in df.columns if x not in ignore],\n",
    "                            outputCol='features')\n",
    "# creating the normalization for all features for scaling betwen 0 to 1\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "# Defining the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, normalizer])\n",
    "# Fitting the pipeline\n",
    "transformations=pipeline.fit(df_imputed)\n",
    "# Applying the transformation\n",
    "df_updated = transformations.transform(df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.49856764446169394\n",
      "Cluster Centers: \n",
      "[9.87267926e-02 5.52420719e-05 4.51923000e-02 1.56132502e-01\n",
      " 1.67607737e-01 3.68861785e-02 8.98299971e-02 4.07539038e-05\n",
      " 1.58877727e-03 1.27030958e-04 1.35435512e-03 8.20280322e-02\n",
      " 3.19938943e-01 1.37602987e-05 2.00543992e-05 4.36753645e-04\n",
      " 2.07896083e-05]\n",
      "[1.56952070e-02 1.07504143e-04 2.60219527e-02 9.19627812e-02\n",
      " 4.12198014e-02 4.13390469e-02 4.21922052e-02 8.65612107e-05\n",
      " 2.36741330e-03 1.49505893e-04 1.69559052e-03 6.73144634e-02\n",
      " 6.69692711e-01 5.20319817e-05 2.03653144e-05 7.78628562e-05\n",
      " 4.99595698e-06]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1003)\n",
    "model = kmeans.fit(df_updated.select('normFeatures').withColumnRenamed('normFeatures','features'))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(df_updated.select('normFeatures').withColumnRenamed('normFeatures','features'))\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sil_coeff=[]\n",
    "num_clusters=[]\n",
    "for iter in range(2,8):\n",
    "    kmeans = KMeans().setK(iter).setSeed(1003)\n",
    "    model = kmeans.fit(df_updated.select('normFeatures').withColumnRenamed('normFeatures','features'))\n",
    "    # Make predictions\n",
    "    predictions = model.transform(df_updated.select('normFeatures').withColumnRenamed('normFeatures','features'))\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    sil_coeff.append(silhouette)\n",
    "    num_clusters.append(iter)\n",
    "    print(\"Silhouette with squared euclidean distance for \"+str(iter) +\" cluster solution = \" + str(silhouette))\n",
    " \n",
    "\n",
    "df_viz=pd.DataFrame(zip(num_clusters,sil_coeff), columns=['num_clusters','silhouette_score'])\n",
    "sns.lineplot(x = \"num_clusters\", y = \"silhouette_score\", data=df_viz)\n",
    "plt.title('k-means : Silhouette scores')\n",
    "plt.xticks(range(2, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
